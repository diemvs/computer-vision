. **Экспорт простой модели PyTorch в ONNX**  
   - Возьмите любую доступную «игрушечную» сверточную сеть или MLP, обученную на PyTorch (можно даже без реального обучения — главное, чтобы были заданы слои).  
   - Подготовьте скрипт на Python, который:  
     1) Создаёт (или загружает) модель.  
     2) Переводит её в режим `eval()`.  
     3) Экспортирует в формат ONNX при помощи `torch.onnx.export`.  
   - Проверьте, что получился корректный `.onnx`-файл (можно использовать `onnx.checker.check_model` или `onnxruntime.InferenceSession` для быстрой валидации).

2. **Сборка TensorRT engine в режиме FP32 и FP16**  
   - Напишите Python-скрипт, где:  
     1) Подключается `tensorrt` (`import tensorrt as trt`).  
     2) Создаётся `Builder`, `Network` (с `EXPLICIT_BATCH`) и `OnnxParser`.  
     3) Загружается ранее полученный `.onnx`-файл и создаётся engine в режиме FP32.  
     4) То же самое, но с установкой флага `BuilderFlag.FP16`, чтобы получить FP16-движок.  
   - Сериализуйте оба варианта engine в файлы (например, `model_fp32.plan` и `model_fp16.plan`) и зафиксируйте их размеры (байты, мегабайты).

3. **Замер производительности**  
   - Для каждого engine (FP32 и FP16) создайте функцию инференса на Python:  
     1) Загрузите engine (через `Runtime` → `deserialize_cuda_engine`).  
     2) Создайте `ExecutionContext`.  
     3) Сформируйте тестовый входной массив (например, случайный тензор подходящей формы, `np.random.randn(...)`).  
     4) Выполните несколько сотен прогонов инференса в цикле и замерьте среднее время (можно с помощью `time.time()` или более точного `time.perf_counter()`).  
   - Сравните, какой движок (FP32 или FP16) быстрее. Замерьте пропускную способность (количество инференсов в секунду) либо усреднённую задержку в миллисекундах.

4. **Эксперимент с INT8 (необязательно)**  
   - Если остаётся время, добавьте шаг калибровки:  
     1) Создайте небольшой класс-калибратор, унаследованный от `trt.IInt8EntropyCalibrator2` (или `trt.IInt8MinMaxCalibrator`).  
     2) Соберите модель в режиме INT8 (`config.set_flag(trt.BuilderFlag.INT8)` и задайте `config.int8_calibrator = ...`).  
     3) Сравните точность (или хотя бы выходы) с FP32, а также замерьте скорость.  
   - Отметьте, сильно ли изменилась метрика по сравнению с FP16/FP32.

5. **Мини-бенчмарк vs PyTorch**  
   - Для полноты эксперимента можно замерить время инференса «как есть» в PyTorch (`model(input_torch)`) на GPU (без дополнительной оптимизации).  
   - Сопоставьте результаты с TensorRT FP32/FP16/INT8, чтобы увидеть реальный прирост производительности.